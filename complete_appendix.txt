APPENDIX A: CODE AND DATA REPOSITORY

A.1 Overview

This appendix provides comprehensive documentation of all code, data, and analytical procedures used in this research to ensure full reproducibility. All materials are organised to enable independent replication of results and extension of the research methodology.

A.2 System Architecture and Core Components

A.2.1 Main System Files

Primary Analysis System:

# gse_sentiment_analysis_system.py - Main sentiment analysis engine
# Implements comprehensive sentiment analysis and prediction pipeline
# Including data collection, processing, ML modeling, and prediction generation

Data Collection Components:

# news_scraper.py - Automated news article collection
# social_media_scraper.py - Social media data gathering
# manual_sentiment_interface.py - Expert input collection system

Data Processing and Loading:

# gse_data_loader.py - Centralized data loading and preprocessing
# add_sample_data.py - Sample data generation for testing

System Setup and Deployment:

# setup_and_run.py - Complete system initialization
# working_dashboard.py - Web-based dashboard implementation
# test_system.py - Comprehensive system testing framework

A.2.2 Configuration and Dependencies

Configuration Management:

# config.json - System configuration parameters
{
  "data_sources": {
    "news_sources": ["ghanaweb", "myjoyonline", "citifm", "joynews", "graphic"],
    "social_platforms": ["twitter", "facebook", "linkedin", "reddit"],
    "update_frequency": "daily"
  },
  "sentiment_analysis": {
    "models": ["vader", "textblob", "custom_ml"],
    "confidence_threshold": 0.7,
    "ensemble_weights": [0.4, 0.3, 0.3]
  },
  "prediction_models": {
    "algorithms": ["xgboost", "lstm", "catboost"],
    "validation_method": "time_series_split",
    "test_size": 0.2
  }
}

Dependencies and Requirements:

# requirements.txt - Python package dependencies
streamlit>=1.28.0
pandas>=1.5.0
numpy>=1.24.0
scikit-learn>=1.3.0
xgboost>=1.7.0
tensorflow>=2.13.0
catboost>=1.2.0
beautifulsoup4>=4.12.0
requests>=2.31.0
nltk>=3.8.0
textblob>=0.17.0
vaderSentiment>=3.3.2
plotly>=5.15.0
yfinance>=0.2.0
sqlite3

A.3 Data Sources and Collection Procedures

A.3.1 News Data Collection

News Sources Configuration:

1. GhanaWeb (www.ghanaweb.com) - Business and finance sections
2. MyJoyOnline (www.myjoyonline.com) - Market news and analysis
3. Citi FM (citifmonline.com) - Financial news coverage
4. Joy News (www.joynews.com) - Economic reporting
5. Graphic Online (www.graphic.com.gh) - Business section
6. Daily Graphic - Financial supplements

Collection Methodology:

# News scraping parameters
SCRAPING_CONFIG = {
    'request_delay': 2.0, # seconds between requests
    'retry_attempts': 3,
    'timeout': 30,
    'user_agent': 'GSE Research Bot 1.0',
    'respect_robots_txt': True,
    'content_filters': ['finance', 'stock', 'investment', 'GSE', 'market']
}

A.3.2 Social Media Data Collection

Platform-Specific Collection:

- Twitter/X API: Real-time tweets with financial keywords
- Facebook Graph API: Public posts from financial pages
- LinkedIn API: Professional network discussions
- Reddit API: Investment community discussions

Keyword Filtering:

FINANCIAL_KEYWORDS = [
    'GSE', 'Ghana Stock Exchange', 'Accra bourse',
    'GCB Bank', 'MTN Ghana', 'Ecobank', 'CalBank',
    'stock price', 'investment', 'dividend', 'earnings',
    'financial results', 'market analysis', 'trading volume'
]

A.3.3 Data Storage Structure

Database Schema:

-- News Articles Table
CREATE TABLE news_articles (
    article_id INTEGER PRIMARY KEY,
    source TEXT NOT NULL,
    title TEXT NOT NULL,
    content TEXT NOT NULL,
    publication_date DATETIME NOT NULL,
    url TEXT UNIQUE,
    sentiment_score REAL,
    relevance_score REAL,
    company_mentions TEXT,
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP
);

-- Social Media Posts Table
CREATE TABLE social_posts (
    post_id INTEGER PRIMARY KEY,
    platform TEXT NOT NULL,
    content TEXT NOT NULL,
    author TEXT,
    post_date DATETIME NOT NULL,
    engagement_metrics TEXT,
    sentiment_score REAL,
    relevance_score REAL,
    company_mentions TEXT,
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP
);

-- Expert Inputs Table
CREATE TABLE expert_inputs (
    input_id INTEGER PRIMARY KEY,
    expert_type TEXT NOT NULL,
    company_symbol TEXT NOT NULL,
    sentiment_score REAL NOT NULL,
    confidence_level REAL NOT NULL,
    commentary TEXT,
    input_date DATETIME NOT NULL,
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP
);

-- Stock Price Data Table
CREATE TABLE stock_prices (
    price_id INTEGER PRIMARY KEY,
    company_symbol TEXT NOT NULL,
    date DATE NOT NULL,
    open_price REAL,
    close_price REAL,
    high_price REAL,
    low_price REAL,
    volume INTEGER,
    price_change REAL,
    percent_change REAL,
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP
);

A.4 Sentiment Analysis Implementation

A.4.1 Hybrid Sentiment Analysis Framework

Multi-Model Approach:

Class HybridSentimentAnalyzer:
    def __init__(self):
        self.vader_analyzer = SentimentIntensityAnalyzer()
        self.textblob_analyzer = TextBlob
        self.custom_model = self.load_custom_model()

    def analyze_sentiment(self, text):
        # VADER Analysis
        vader_scores = self.vader_analyzer.polarity_scores(text)
        vader_compound = vader_scores['compound']

        # TextBlob Analysis
        blob = TextBlob(text)
        textblob_polarity = blob.sentiment.polarity

        # Custom ML Model
        custom_prediction = self.custom_model.predict([text])[0]

        # Ensemble Combination
        ensemble_score = (
            0.4 * vader_compound +
            0.3 * textblob_polarity +
            0.3 * custom_prediction
        )

        return {
            'vader': vader_compound,
            'textblob': textblob_polarity,
            'custom': custom_prediction,
            'ensemble': ensemble_score,
            'confidence': self.calculate_confidence(vader_compound, textblob_polarity, custom_prediction)
        }

A.4.2 Feature Engineering for ML Models

Sentiment Features:

def create_sentiment_features(sentiment_data):
    features = {}

    # Basic sentiment metrics
    features['current_sentiment'] = sentiment_data['ensemble']
    features['sentiment_magnitude'] = abs(sentiment_data['ensemble'])

    # Temporal features
    features['sentiment_momentum'] = calculate_momentum(sentiment_data, window=3)
    features['sentiment_volatility'] = calculate_volatility(sentiment_data, window=7)
    features['sentiment_trend'] = calculate_trend(sentiment_data, window=5)

    # Source-weighted features
    features['news_sentiment'] = weight_by_source(sentiment_data, 'news')
    features['social_sentiment'] = weight_by_source(sentiment_data, 'social')
    features['expert_sentiment'] = weight_by_source(sentiment_data, 'expert')

    # Technical indicators
    features['sentiment_rsi'] = calculate_sentiment_rsi(sentiment_data, window=14)
    features['sentiment_sma'] = calculate_sma(sentiment_data, window=5)
    features['sentiment_ema'] = calculate_ema(sentiment_data, window=5)

    return features

A.5 Machine Learning Model Implementation

A.5.1 XGBoost Implementation

Optimal Hyperparameters:

XGBOOST_PARAMS = {
    'n_estimators': 500,
    'max_depth': 6,
    'learning_rate': 0.1,
    'subsample': 0.8,
    'colsample_bytree': 0.8,
    'gamma': 0.1,
    'min_child_weight': 1
    'reg_alpha': 0.01,
    'reg_lambda': 0.01,
    'random_state': 42,
    'n_jobs': -1
}

def train_xgboost_model(X_train, y_train, X_val, y_val):
    model = XGBClassifier(**XGBOOST_PARAMS)

    model. fit(
        X_train, y_train,
        eval_set=[(X_val, y_val)],
        early_stopping_rounds=50,
        verbose=False
    )

    return model

A.5.2 LSTM Neural Network Implementation

Architecture and Training:

def create_lstm_model(sequence_length, n_features):
    model = Sequential([
        LSTM(128, return_sequences=True, input_shape=(sequence_length, n_features)),
        Dropout(0.3),
        LSTM(64, return_sequences=False),
        Dropout(0.3),
        Dense(32, activation='relu'),
        Dropout(0.2),
        Dense(1, activation='sigmoid')
    ])

    model. compile(
        optimizer=Adam(learning_rate=0.001),
        loss='binary_crossentropy',
        metrics=['accuracy', 'precision', 'recall']
    )

    return model

LSTM_TRAINING_PARAMS = {
    'epochs': 100,
    'batch_size': 32,
    'validation_split': 0.2,
    'early_stopping_patience': 15,
    'reduce_lr_patience': 10
}

A.5.3 Ensemble Model Implementation

Weighted Voting Ensemble:

class WeightedEnsemble:
    def __init__(self, models, weights):
        self.models = models
        self.weights = weights / np.sum(weights)  # Normalize weights

    def predict_proba(self, X):
        predictions = np.zeros((X.shape[0], 2))

        for model, weight in zip(self.models, self.weights):
            model_pred = model.predict_proba(X)
            predictions += weight * model_pred

        return predictions

    def predict(self, X):
        probabilities = self.predict_proba(X)
        return (probabilities[:, 1] > 0.5).astype(int)

# Ensemble weights based on validation performance
ENSEMBLE_WEIGHTS = [0.4, 0.35, 0.25]  # XGBoost, LSTM, CatBoost

A.6 Evaluation and Validation Procedures

A.6.1 Time Series Cross-Validation

Walk-Forward Validation Implementation:

def time_series_cross_validation(X, y, dates, n_splits=5):
    """
    Implements walk-forward validation for time series data
    """
    total_samples = len(X)
    test_size = total_samples // (n_splits + 1)

    for i in range(n_splits):
        train_end = total_samples - (n_splits - i) * test_size
        test_start = train_end
        test_end = test_start + test_size

        train_indices = range(0, train_end)
        test_indices = range(test_start, test_end)

        X_train = X.iloc[train_indices]
        X_test = X.iloc[test_indices]
        y_train = y.iloc[train_indices]
        y_test = y.iloc[test_indices]

        yield X_train, X_test, y_train, y_test

A.6.2 Performance Metrics Calculation

Comprehensive Evaluation Framework:

def calculate_performance_metrics(y_true, y_pred, y_proba):
    """
    Calculate comprehensive performance metrics.
    """
    metrics = {}

    # Basic classification metrics
    metrics['accuracy'] = accuracy_score(y_true, y_pred)
    metrics['precision'] = precision_score(y_true, y_pred)
    metrics['recall'] = recall_score(y_true, y_pred)
    metrics['f1_score'] = f1_score(y_true, y_pred)
    metrics['specificity'] = recall_score(y_true, y_pred, pos_label=0)

    # Probabilistic metrics
    metrics['auc_roc'] = roc_auc_score(y_true, y_proba[:, 1])
    metrics['auc_pr'] = average_precision_score(y_true, y_proba[:, 1])
    metrics['log_loss'] = log_loss(y_true, y_proba)

    # Confidence intervals (95%)
    n = len(y_true)
    for metric_name in ['accuracy', 'precision', 'recall', 'f1_score']:
        value = metrics[metric_name]
        se = np.sqrt((value * (1 - value)) / n)
        ci_lower = value - 1.96 * se
        ci_upper = value + 1.96 * se
        metrics[f'{metric_name}_ci_lower'] = ci_lower
        metrics[f'{metric_name}_ci_upper'] = ci_upper

    return metrics

A.7 Statistical Analysis Procedures

A.7.1 Granger Causality Testing

Implementation of Econometric Tests:

from statsmodels. tsa.stattools import grangercausalitytests, adfuller
from statsmodels.stats.diagnostic import acorr_ljungbox

def perform_granger_causality_analysis(sentiment_series, price_series, max_lags=5):
    """
    Comprehensive Granger causality analysis
    """
    results = {}

    # Stationarity testing
    sentiment_adf = adfuller(sentiment_series)
    price_adf = adfuller(price_series)

    results['sentiment_stationarity'] = {
        'adf_statistic': sentiment_adf[0],
        'p_value': sentiment_adf[1],
        'is_stationary': sentiment_adf[1] < 0.05
    }

    results['price_stationarity'] = {
        'adf_statistic': price_adf[0],
        'p_value': price_adf[1],
        'is_stationary': price_adf[1] < 0.05
    }

    # Prepare data for Granger causality
    data = pd.DataFrame({
        'sentiment': sentiment_series,
        'price_change': price_series
    }).dropna()

    # Perform Granger causality test
    gc_results = grangercausalitytests(data[['price_change', 'sentiment']], max_lags, verbose=False)

    # Extract results
    results['granger_causality'] = {}
    for lag in range(1, max_lags + 1):
        f_stat = gc_results[lag][0]['ssr_ftest'][0]
        p_value = gc_results[lag][0]['ssr_ftest'][1]
        results['granger_causality'][lag] = {
            'f_statistic': f_stat,
            'p_value': p_value,
            'is_significant': p_value < 0.05
        }

    return results

A.7.2 Correlation Analysis with Confidence Intervals

Bootstrap Confidence Intervals:

def calculate_correlation_with_ci(x, y, n_bootstrap=1000, alpha=0.05):
    """
    Calculate correlation with bootstrap confidence intervals
    """
    from scipy.stats import pearsonr, spearmanr

    # Original correlation
    pearson_r, pearson_p = pearsonr(x, y)
    spearman_r, spearman_p = spearmanr(x, y)

    # Bootstrap resampling
    n_samples = len(x)
    bootstrap_correlations = []

    for _ in range(n_bootstrap):
        indices = np.random.choice(n_samples, n_samples, replace=True)
        x_boot = x.iloc[indices] if hasattr(x, 'iloc') else x[indices]
        y_boot = y.iloc[indices] if hasattr(y, 'iloc') else y[indices]

        r_boot, _ = pearsonr(x_boot, y_boot)
        bootstrap_correlations.append(r_boot)

    # Calculate confidence intervals
    ci_lower = np.percentile(bootstrap_correlations, 100 * alpha / 2)
    ci_upper = np.percentile(bootstrap_correlations, 100 * (1 - alpha / 2))

    return {
        'pearson_r': pearson_r,
        'pearson_p': pearson_p,
        'spearman_r': spearman_r,
        'spearman_p': spearman_p,
        'ci_lower': ci_lower,
        'ci_upper': ci_upper,
        'bootstrap_samples': bootstrap_correlations
    }

A.8 Data Files and Datasets

A.8.1 Primary Data Files

Core Datasets: 1. GSE COMPOSITE INDEX.csv - Historical GSE composite index data
2. GSE FINANCIAL INDEX.csv - Financial sector index data
3. GSE_Company_List_and_Keywords.xlsx - Company information and keywords
4. GSE_Investor_Model.xlsx - Investment analysis model
 5. gse_sentiment.db - SQLite database with all collected sentiment data

Database Contents:
- News Articles: 3,147 articles from 6 sources (Jan 2023 - Dec 2024)
- Social Media Posts: 17,124 posts from 4 platforms
- Expert Inputs: 47 expert sentiment assessments
- Stock Price Data: Daily price data for 18 GSE-listed companies
- Processed Features: Engineered features for ML model training

A.8.2 Data Processing Scripts

Data Preprocessing Pipeline:

# Data cleaning and preprocessing
def preprocess_data():
    # Load raw data
    news_data = load_news_data()
    social_data = load_social_data()
    price_data = load_price_data()

    # Clean text data
    news_data['content_clean'] = news_data['content'].apply(clean_text)
    social_data['content_clean'] = social_data['content'].apply(clean_text)

    # Calculate sentiment scores
    news_data['sentiment'] = news_data['content_clean'].apply(analyze_sentiment)
    social_data['sentiment'] = social_data['content_clean'].apply(analyze_sentiment)

    # Aggregate daily sentiment
    daily_sentiment = aggregate_daily_sentiment(news_data, social_data)

    # Merge with price data
    model_data = merge_sentiment_price_data(daily_sentiment, price_data)

    # Feature engineering
    model_data = create_features(model_data)

    return model_data

def clean_text(text):
    """Text preprocessing for sentiment analysis"""
    # Remove URLs, mentions, hashtags
    text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)
    text = re.sub(r'@\w+|#\w+', '', text)

    # Remove special characters and digits
    text = re.sub(r'[^a-zA-Z\s]', '', text)

    # Convert to lowercase and strip whitespace
    text = text.lower().strip()

    return text

A.9 Model Training and Validation Scripts

A.9.1 Complete Training Pipeline

Full Model Training Script:

def run_complete_analysis():
    """
    Complete analysis pipeline from data loading to model evaluation
    """
    print("Starting GSE Sentiment Analysis Pipeline...")

    # 1. Data Loading and Preprocessing
    print("Loading and preprocessing data...")
    data = preprocess_data()

    # 2. Feature Engineering
    print("Engineering features...")
    features = create_all_features(data)

    # 3. Train-Test Split (Time-based)
    print("Splitting data temporally...")
    X_train, X_test, y_train, y_test = temporal_train_test_split(features)

    # 4. Model Training
    print("Training models...")
    models = {}

    # XGBoost
    print("  Training XGBoost...")
    models['xgboost'] = train_xgboost_model(X_train, y_train, X_test, y_test)

    # LSTM
    print("  Training LSTM...")
    models['lstm'] = train_lstm_model(X_train, y_train, X_test, y_test)

    # CatBoost
    print("  Training CatBoost...")
    models['catboost'] = train_catboost_model(X_train, y_train, X_test, y_test)

    # 5. Ensemble Creation
    print("Creating ensemble model...")
    ensemble_model = create_ensemble(models, X_test, y_test)

    # 6. Model Evaluation
    print("Evaluating models...")
    results = {}
    for name, model in models.items():
        y_pred = model.predict(X_test)
        y_proba = model.predict_proba(X_test)
        results[name] = calculate_performance_metrics(y_test, y_pred, y_proba)

    # Ensemble evaluation
    y_pred_ensemble = ensemble_model.predict(X_test)
    y_proba_ensemble = ensemble_model.predict_proba(X_test)
    results['ensemble'] = calculate_performance_metrics(y_test, y_pred_ensemble, y_proba_ensemble)

    # 7. Statistical Analysis
    print("Performing statistical analysis...")
    correlation_results = perform_correlation_analysis(data)
    granger_results = perform_granger_analysis(data)

    # 8. Save Results
    print("Saving results...")
    save_model_results(models, results, correlation_results, granger_results)

    print("Analysis complete!")

    return models, results, correlation_results, granger_results

A.9.2 Cross-Validation Implementation

Robust Cross-Validation Framework:

def perform_cross_validation(X, y, dates, models, cv_folds=5):
    """
    Perform time-series cross-validation for all models.
    """
    cv_results = {model_name: [] for model_name in models.keys()}

    # Time series cross-validation
    for fold, (train_idx, test_idx) in enumerate(time_series_cross_validation(X, y, dates, cv_folds)):
        print(f"Cross-validation fold {fold + 1}/{cv_folds}")

        X_train_cv = X.iloc[train_idx]
        X_test_cv = X.iloc[test_idx]
        y_train_cv = y.iloc[train_idx]
        y_test_cv = y.iloc[test_idx]

        for model_name, model_class in models.items():
            # Train model
            model = model_class()
            model.fit(X_train_cv, y_train_cv)

            # Evaluate
            y_pred = model.predict(X_test_cv)
            y_proba = model.predict_proba(X_test_cv)
            metrics = calculate_performance_metrics(y_test_cv, y_pred, y_proba)
            cv_results[model_name].append(metrics)

    # Calculate mean and std for each metric
    final_results = {}
    for model_name in models.keys():
        final_results[model_name] = {}
        for metric in cv_results[model_name][0].keys():
            values = [fold_result[metric] for fold_result in cv_results[model_name]]
            final_results[model_name][f'{metric}_mean'] = np.mean(values)
            final_results[model_name][f'{metric}_std'] = np.std(values)

    return final_results

A.10 Deployment and Production Code

A.10.1 Streamlit Dashboard Implementation

Web Application Framework:

# working_dashboard.py - Main dashboard application
import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go

def main():
    st.set_page_config(
        page_title="GSE Sentiment Analysis Dashboard",
        page_icon="📈",
        layout="wide"
    )

    st.title("🇬🇭 Ghana Stock Exchange Sentiment Analysis Dashboard")
    st.sidebar.title("Navigation")

    # Sidebar navigation
    page = st.sidebar.selectbox(
        "Choose Analysis",
        ["Real-time Analysis", "Historical Performance", "Model Insights", "Expert Input"]
    )

    if page == "Real-time Analysis":
        show_realtime_analysis()
    elif page == "Historical Performance":
        show_historical_performance()
    elif page == "Model Insights":
        show_model_insights()
    elif page == "Expert Input":
        show_expert_input()

def show_realtime_analysis():
    """Display real-time sentiment analysis"""
    st.header("Real-time Sentiment Analysis")

    # Load latest data
    latest_sentiment = load_latest_sentiment_data()
    latest_predictions = generate_latest_predictions()

    # Display current sentiment
    col1, col2, col3 = st.columns(3)

    with col1:
        st.metric(
            "Overall Sentiment",
            f"{latest_sentiment['overall']:.3f}",
            f"{latest_sentiment['change']:.3f}"
        )

    with col2:
        st.metric(
            "News Sentiment",
            f"{latest_sentiment['news']:.3f}",
            f"{latest_sentiment['news_change']:.3f}"
        )

    with col3:
        st.metric(
            "Social Sentiment",
            f"{latest_sentiment['social']:.3f}",
            f"{latest_sentiment['social_change']:.3f}"
        )

    # Predictions table
    st.subheader("Stock Price Predictions")
    st.dataframe(latest_predictions)

    # Sentiment trends
    st.subheader("Sentiment Trends")
    sentiment_chart = create_sentiment_trend_chart()
    st.plotly_chart(sentiment_chart, use_container_width=True)

if __name__ == "__main__":
    main()

A.10.2 Automated Data Collection Scripts

Production Data Collection:

# Automated data collection scheduler
def run_daily_data_collection():
    """
    Daily automated data collection and processing
    """
    try:
        print(f"Starting daily data collection: {datetime.now()}")

        # Collect news articles
        news_collector = NewsCollector()
        new_articles = news_collector.collect_daily_articles()
        print(f"Collected {len(new_articles)} new articles")

        # Collect social media posts
        social_collector = SocialMediaCollector()
        new_posts = social_collector.collect_daily_posts()
        print(f"Collected {len(new_posts)} new social media posts")

        # Process sentiment
        sentiment_processor = SentimentProcessor()
        news_sentiment = sentiment_processor.process_news(new_articles)
        social_sentiment = sentiment_processor.process_social(new_posts)

        # Update database
        database_manager = DatabaseManager()
        database_manager.insert_news_data(new_articles, news_sentiment)
        database_manager.insert_social_data(new_posts, social_sentiment)

        # Generate new predictions
        predictor = StockPredictor()
        predictions = predictor.generate_daily_predictions()
        database_manager.insert_predictions(predictions)

        print("Daily data collection completed successfully")

    except Exception as e:
        print(f"Error in daily data collection: {str(e)}")
        # Send alert notification
        send_error_notification(str(e))

A.11 Replication Instructions

A.11.1 System Setup

Complete Installation Guide:

# 1. Clone the repository
git clone https://github.com/gse-sentiment-analysis/research-code.git
cd research-code

# 2. Create virtual environment
python -m venv gse_sentiment_env
source gse_sentiment_env/bin/activate  # On Windows: gse_sentiment_env\Scripts\activate

# 3. Install dependencies
pip install -r requirements.txt

# 4. Download NLTK data
python -c "import nltk; nltk.download('vader_lexicon'); nltk.download('punkt')"

# 5. Set up configuration
cp config.json.example config.json
# Edit config.json with your API keys and preferences

# 6. Initialize database
python setup_and_run.py --init-db

# 7. Run initial data collection (optional, uses sample data if skipped)
python setup_and_run.py --collect-data

# 8. Train models
python setup_and_run.py --train-models

# 9. Start dashboard
streamlit run working_dashboard.py

A.11.2 Reproducing Research Results

Step-by-Step Replication:

# Complete research replication script
def reproduce_research_results():
    """
    Reproduce all research results presented in Chapter 4
    """
    print("=== GSE Sentiment Analysis Research Replication ===")

    # 1. Data Collection (if not using provided data)
    print("\n1. Data Collection Phase")
    if use_provided_data:
        load_provided_datasets()
    else:
        run_full_data_collection()

    # 2. Sentiment Analysis
    print("\n2. Sentiment Analysis Phase")
    sentiment_results = perform_sentiment_analysis()
    validate_sentiment_results(sentiment_results)

    # 3. Machine Learning Models
    print("\n3. Machine Learning Phase")
    ml_results = train_and_evaluate_models()

    # 4. Statistical Analysis
    print("\n4. Statistical Analysis Phase")
    correlation_results = perform_correlation_analysis()
    granger_results = perform_granger_causality_tests()

    # 5. Performance Evaluation
    print("\n5. Performance Evaluation Phase")
    performance_results = evaluate_prediction_performance()

    # 6. Generate Tables and Figures
    print("\n6. Generating Research Outputs")
    generate_research_tables(ml_results, correlation_results, performance_results)
    generate_research_figures(sentiment_results, ml_results, correlation_results)

    # 7. Validation
    print("\n7. Validating Results")
    validate_against_published_results(
        ml_results, correlation_results, performance_results
    )

    print("\n=== Replication Complete ===")
    print("All results have been reproduced and validated.")
    print("Generated files:")
    print("- results/model_performance_table.csv")
    print("- results/correlation_analysis_table.csv")
    print("- results/prediction_accuracy_table.csv")
    print("- figures/sentiment_distribution.png")
    print("- figures/model_comparison.png")
    print("- figures/correlation_heatmap.png")

    return {
        'ml_results': ml_results,
        'correlation_results': correlation_results,
        'performance_results': performance_results
    }

# Run replication
if __name__ == "__main__":
    results = reproduce_research_results()

A.11.3 Data Availability Statement

Data Access and Usage:

Data Availability Statement:

1. Raw Data:
   - News articles: Available in gse_sentiment.db (anonymized content)
   - Social media posts: Available in gse_sentiment.db (public posts only)
   - Stock price data: Available in GSE_COMPOSITE_INDEX.csv and GSE_FINANCIAL_INDEX.csv
   - Expert inputs: Available in gse_sentiment.db (anonymized)

2. Processed Data:
   - Sentiment scores: Calculated using provided scripts
   - Feature matrices: Generated using provided feature engineering code
   - Model inputs: Created through preprocessing pipeline

3. Code Availability:
   - All analysis code: Provided in repository
   - Model training scripts: Complete implementation included
   - Evaluation frameworks: Full replication possible

4. Reproducibility:
   - Random seeds: Fixed for reproducible results
   - Version control: All dependency versions specified
   - Documentation: Complete setup and execution instructions

5. Ethical Considerations:
   - Personal information: Removed from all datasets
   - Copyright compliance: Only public/authorized content included
   - Terms of service: All data collection respects platform terms

This comprehensive appendix ensures complete reproducibility of the research findings and provides all necessary components for independent validation and extension of the work.

APPENDIX B: EDA AND FEATURE SELECTION CODE

B.1 Data Analysis Script (analyze_data.py)

```python
#!/usr/bin/env python3
"""
GSE Sentiment Analysis EDA and Feature Selection Pipeline
Comprehensive exploratory data analysis and feature selection for GSE sentiment data

To run this script:
1. Ensure you have the required data files in the same directory:
   - gse_sentiment.db (SQLite database with sentiment data)
   - GSE COMPOSITE INDEX.csv (stock market data)

2. Install required packages:
   pip install pandas numpy matplotlib seaborn scikit-learn scipy

3. Run the script:
   python analyze_data.py

Full repository: https://github.com/OforiPrescott/Leveraging-Big-Data-Analytics-to-Inform-Investor-Decision-on-the-GSE
"""

import pandas as pd
import numpy as np
import sqlite3
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.feature_selection import RFE, mutual_info_regression
from sklearn.preprocessing import StandardScaler
from scipy.stats import pearsonr, spearmanr
import json
import os
from datetime import datetime

# Set style for plots
plt.style.use('default')
sns.set_palette("husl")

class GSEDataAnalyzer:
    """Comprehensive EDA and feature selection for GSE sentiment analysis"""

    def __init__(self, db_path="gse_sentiment.db"):
        self.db_path = db_path
        self.sentiment_df = None
        self.stock_df = None
        self.merged_df = None

    def load_data(self):
        """Load sentiment and stock data from database and CSV files"""
        print("Loading data...")

        # Load sentiment data
        conn = sqlite3.connect(self.db_path)
        self.sentiment_df = pd.read_sql_query("""
            SELECT timestamp, source, sentiment_score, sentiment_label,
                   company, confidence, content
             FROM sentiment_data
        """, conn)
        conn.close()

        # Load stock data
        self.stock_df = pd.read_csv("GSE COMPOSITE INDEX.csv",
                                    header=None, skiprows=1)
        self.stock_df.columns = ['Date', 'Open', 'High', 'Low', 'Close',
                                'Turnover', 'Adj Close', 'Trades']

        # Clean and process data
        self._clean_data()

        print(f"Loaded {len(self.sentiment_df)} sentiment entries")
        print(f"Loaded {len(self.stock_df)} stock records")

    def _clean_data(self):
        """Clean and preprocess the data"""

        # Process sentiment data
        self.sentiment_df['timestamp'] = pd.to_datetime(
            self.sentiment_df['timestamp'], errors='coerce'
        )
        self.sentiment_df['date'] = self.sentiment_df['timestamp'].dt.date
        self.sentiment_df['date'] = pd.to_datetime(self.sentiment_df['date'])

        # Process stock data
        self.stock_df['Date'] = pd.to_datetime(
            self.stock_df['Date'], format='%d/%m/%Y', errors='coerce'
        )
        self.stock_df = self.stock_df.dropna(subset=['Date'])

        # Calculate target variable (next day price movement)
        self.stock_df = self.stock_df.sort_values('Date')
        self.stock_df['Price_Change'] = self.stock_df['Close'].pct_change()
        self.stock_df['Target'] = (self.stock_df['Price_Change'].shift(-1) > 0).astype(int)

        # Add technical indicators
        self._add_technical_indicators()

    def _add_technical_indicators(self):
        """Add technical indicators to stock data"""

        df = self.stock_df

        # Moving averages
        df['MA_5'] = df['Close'].rolling(window=5).mean()
        df['MA_10'] = df['Close'].rolling(window=10).mean()
        df['MA_20'] = df['Close'].rolling(window=20).mean()

        # Price changes
        df['Price_Change_1d'] = df['Close'].pct_change(1)
        df['Price_Change_5d'] = df['Close'].pct_change(5)

        # Volume indicators
        df['Volume_MA'] = df['Turnover'].rolling(window=10).mean()
        df['Volume_Ratio'] = df['Turnover'] / df['Volume_MA']

        # RSI (simplified)
        df['RSI'] = 50  # Placeholder for RSI calculation

    def perform_eda(self):
        """Perform comprehensive exploratory data analysis"""

        print("\n=== EXPLORATORY DATA ANALYSIS ===")

        # Sentiment analysis
        self._analyze_sentiment_data()

        # Stock data analysis
        self._analyze_stock_data()

        # Correlation analysis
        self._analyze_correlations()

        # Time series analysis
        self._analyze_time_series()

    def _analyze_sentiment_data(self):
        """Analyze sentiment data characteristics"""

        print("\n1. SENTIMENT DATA ANALYSIS")
        print("-" * 40)

        df = self.sentiment_df

        print(f"Total sentiment entries: {len(df)}")
        print(f"Date range: {df['timestamp'].min()} to {df['timestamp'].max()}")
        print(f"Companies covered: {df['company'].nunique()}")
        print(f"Sources: {df['source'].nunique()}")

        # Sentiment distribution
        sentiment_dist = df['sentiment_label'].value_counts(normalize=True)
        print("\nSentiment distribution:")
        for label, pct in sentiment_dist.items():
            print(f"  {label}: {pct:.1%}")

        # Sentiment statistics
        print(f"\nSentiment score statistics:")
        print(f"  Mean: {df['sentiment_score'].mean():.3f}")
        print(f"  Std: {df['sentiment_score'].std():.3f}")
        print(f"  Range: {df['sentiment_score'].min():.3f} to {df['sentiment_score'].max():.3f}")

    def _analyze_stock_data(self):
        """Analyze stock market data characteristics"""

        print("\n2. STOCK DATA ANALYSIS")
        print("-" * 35)

        df = self.stock_df

        print(f"Total trading records: {len(df)}")
        print(f"Date range: {df['Date'].min()} to {df['Date'].max()}")
        print(f"Price range: {df['Close'].min():.2f} - {df['Close'].max():.2f}")

        # Trading activity
        print(f"Average daily turnover: {df['Turnover'].mean():,.0f}")
        print(f"Average daily price change: {df['Price_Change'].mean():.3f}")

        # Trading by day of week
        df['Day_of_Week'] = df['Date'].dt.day_name()
        trading_by_day = df.groupby('Day_of_Week')['Turnover'].count().sort_values(ascending=False)
        print("\nTrading activity by day:")
        for day, count in trading_by_day.items():
            print(f"  {day}: {count} days")

    def _analyze_correlations(self):
        """Analyze correlations between variables"""

        print("\n3. CORRELATION ANALYSIS")
        print("-" * 25)

        # Merge data for correlation analysis
        self._merge_datasets()

        if self.merged_df is not None and len(self.merged_df) > 0:
            # Calculate correlations
            numeric_cols = self.merged_df.select_dtypes(include=[np.number]).columns
            corr_matrix = self.merged_df[numeric_cols].corr()

            # Target correlations
            target_corr = corr_matrix['Target'].abs().sort_values(ascending=False)
            print("\nTop correlations with target variable:")
            for var, corr in target_corr.head(10).items():
                print(f"  {var}: {corr:.3f}")

    def _analyze_time_series(self):
        """Analyze time series patterns"""

        print("\n4. TIME SERIES ANALYSIS")
        print("-" * 25)

        if self.merged_df is not None:
            # Daily sentiment aggregation
            daily_sentiment = self.merged_df.groupby('Date').agg({
                'sentiment_score': ['mean', 'std', 'count'],
                'Target': 'mean'
            }).fillna(0)

            print(f"Days with sentiment data: {len(daily_sentiment)}")
            print(f"Average daily sentiment: {daily_sentiment[('sentiment_score', 'mean')].mean():.3f}")
            print(f"Daily sentiment volatility: {daily_sentiment[('sentiment_score', 'std')].mean():.3f}")

    def _merge_datasets(self):
        """Merge sentiment and stock data"""

        if self.sentiment_df is None or self.stock_df is None:
            return

        # Aggregate sentiment by date
        sentiment_daily = self.sentiment_df.groupby('date').agg({
            'sentiment_score': ['mean', 'std', 'count'],
            'confidence': 'mean'
        }).fillna(0)

        # Flatten column names
        sentiment_daily.columns = ['sentiment_mean', 'sentiment_std',
                                  'sentiment_count', 'confidence_mean']
        sentiment_daily = sentiment_daily.reset_index()

        # Merge with stock data
        # Convert timestamp to date for merging
        sentiment_daily_copy = sentiment_daily.copy()
        sentiment_daily_copy['date'] = sentiment_daily_copy['date'].dt.date
        sentiment_daily_copy['date'] = pd.to_datetime(sentiment_daily_copy['date'])

        self.merged_df = pd.merge(
            self.stock_df[['Date', 'Close', 'Price_Change', 'Turnover', 'Target',
                          'MA_5', 'MA_10', 'Price_Change_1d', 'Price_Change_5d',
                          'Volume_Ratio', 'RSI']],
            sentiment_daily_copy,
            left_on='Date',
            right_on='date',
            how='left'
        ).fillna(0)

    def perform_feature_selection(self):
        """Perform comprehensive feature selection"""

        print("\n=== FEATURE SELECTION ANALYSIS ===")

        if self.merged_df is None or len(self.merged_df) == 0:
            print("No merged data available for feature selection")
            return

        # Prepare features
        feature_cols = ['sentiment_mean', 'sentiment_std', 'sentiment_count',
                       'confidence_mean', 'MA_5', 'MA_10', 'Price_Change_1d',
                       'Price_Change_5d', 'Volume_Ratio', 'RSI']

        X = self.merged_df[feature_cols].fillna(0)
        y = self.merged_df['Target']

        # Remove rows where target is NaN
        valid_idx = ~y.isna()
        X = X[valid_idx]
        y = y[valid_idx]

        if len(X) == 0:
            print("No valid data for feature selection")
            return

        print(f"Feature selection dataset: {len(X)} samples, {len(feature_cols)} features")

        # 1. Correlation analysis
        self._correlation_analysis(X, y)

        # 2. Mutual information
        self._mutual_information_analysis(X, y)

        # 3. Recursive feature elimination
        self._rfe_analysis(X, y)

        # 4. Random forest importance
        self._rf_importance_analysis(X, y)

    def _correlation_analysis(self, X, y):
        """Perform correlation analysis"""

        print("\n1. CORRELATION WITH TARGET")
        print("-" * 30)

        correlations = {}
        for col in X.columns:
            try:
                corr, _ = pearsonr(X[col], y)
                correlations[col] = abs(corr)
            except:
                correlations[col] = 0

        # Sort by absolute correlation
        sorted_corr = sorted(correlations.items(), key=lambda x: x[1], reverse=True)

        print("\nTop correlated features:")
        for feature, corr in sorted_corr[:5]:
            print(f"  {feature}: {corr:.3f}")

        self.correlation_results = sorted_corr

    def _mutual_information_analysis(self, X, y):
        """Perform mutual information analysis"""

        print("\n2. MUTUAL INFORMATION SCORES")
        print("-" * 32)

        try:
            mi_scores = mutual_info_regression(X, y, random_state=42)

            mi_results = list(zip(X.columns, mi_scores))
            mi_results.sort(key=lambda x: x[1], reverse=True)

            print("\nMutual information scores:")
            for feature, score in mi_results[:10]:
                print(f"  {feature}: {score:.4f}")

            self.mi_results = mi_results

        except Exception as e:
            print(f"Mutual information analysis failed: {e}")
            self.mi_results = []

    def _rfe_analysis(self, X, y):
        """Perform recursive feature elimination"""

        print("\n3. RECURSIVE FEATURE ELIMINATION")
        print("-" * 35)

        try:
            # Use Random Forest for RFE
            rf = RandomForestRegressor(n_estimators=100, random_state=42)
            rfe = RFE(estimator=rf, n_features_to_select=5)
            rfe.fit(X, y)

            selected_features = X.columns[rfe.support_].tolist()
            print(f"\nRFE selected features: {selected_features}")

            self.rfe_features = selected_features

        except Exception as e:
            print(f"RFE analysis failed: {e}")
            self.rfe_features = []

    def _rf_importance_analysis(self, X, y):
        """Perform random forest feature importance analysis"""

        print("\n4. RANDOM FOREST FEATURE IMPORTANCE")
        print("-" * 38)

        try:
            rf = RandomForestRegressor(n_estimators=100, random_state=42)
            rf.fit(X, y)

            importance_results = list(zip(X.columns, rf.feature_importances_))
            importance_results.sort(key=lambda x: x[1], reverse=True)

            print("\nFeature importance scores:")
            for feature, importance in importance_results[:10]:
                print(f"  {feature}: {importance:.4f}")

            self.importance_results = importance_results

        except Exception as e:
            print(f"Random forest importance analysis failed: {e}")
            self.importance_results = []

    def generate_visualizations(self):
        """Generate EDA visualizations"""

        print("\n=== GENERATING VISUALIZATIONS ===")

        # Create output directory
        os.makedirs('eda_plots', exist_ok=True)

        # Sentiment analysis plots
        self._create_sentiment_plots()

        # Stock analysis plots
        self._create_stock_plots()

        # Correlation heatmap
        self._create_correlation_plot()

        print("\nSaved visualizations to eda_plots/ directory")

    def _create_sentiment_plots(self):
        """Create sentiment analysis visualizations"""

        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))

        # Sentiment distribution
        sentiment_counts = self.sentiment_df['sentiment_label'].value_counts()
        sentiment_counts.plot(kind='bar', ax=ax1, color=['red', 'blue', 'green'])
        ax1.set_title('Sentiment Distribution')
        ax1.set_ylabel('Count')

        # Sentiment scores over time
        daily_sentiment = self.sentiment_df.groupby('date')['sentiment_score'].mean()
        daily_sentiment.plot(ax=ax2, color='blue')
        ax2.set_title('Average Daily Sentiment Score')
        ax2.set_ylabel('Sentiment Score')

        # Sentiment by source
        source_sentiment = self.sentiment_df.groupby('source')['sentiment_score'].mean()
        source_sentiment.plot(kind='bar', ax=ax3, color='orange')
        ax3.set_title('Average Sentiment by Source')
        ax3.set_ylabel('Sentiment Score')
        ax3.tick_params(axis='x', rotation=45)

        # Sentiment by company
        company_sentiment = self.sentiment_df.groupby('company')['sentiment_score'].mean()
        company_sentiment.plot(kind='bar', ax=ax4, color='purple')
        ax4.set_title('Average Sentiment by Company')
        ax4.set_ylabel('Sentiment Score')
        ax4.tick_params(axis='x', rotation=45)

        plt.tight_layout()
        plt.savefig('eda_plots/sentiment_analysis.png', dpi=300, bbox_inches='tight')
        plt.close()

    def _create_stock_plots(self):
        """Create stock market analysis visualizations"""

        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))

        # Price over time
        self.stock_df.plot(x='Date', y='Close', ax=ax1, color='blue')
        ax1.set_title('GSE Composite Index Price Over Time')
        ax1.set_ylabel('Price (GHS)')

        # Daily returns distribution
        self.stock_df['Price_Change'].dropna().plot(kind='hist', bins=50, ax=ax2, color='green